{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714dd8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('.', '..')))\n",
    "sys.path.append(os.path.abspath(os.path.join('.', 'src')))\n",
    "sys.path.append(os.path.abspath(os.path.join('.', 'utils')))\n",
    "\n",
    "# import utils.str_manip as str_manip\n",
    "# import utils.feature_builder as feature_builder\n",
    "import src.job_title_clustering.cluster as cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f63729",
   "metadata": {},
   "source": [
    "## Load the original data and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4755316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load existing clusters\n",
    "orig_df = pd.read_csv('../data/original_clusters.csv')\n",
    "# keep only first 2 columns\n",
    "orig_df = orig_df[['Title', 'Cluster']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461cde54",
   "metadata": {},
   "source": [
    "### We can use other classes to perform low-level functionality directly (for testing purposes, etc.), but the Cluster class is already subclassed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee87e9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm = str_manip.StrManip()\n",
    "# fb = feature_builder.FeatureBuilder()\n",
    "cl = cluster.Cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae623679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in the raw data to get out features\n",
    "feats = cl.get_features(orig_df['Title'], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f78855",
   "metadata": {},
   "source": [
    "## Create the processing pipeline\n",
    "### Configurations for all possible combinations of the processing pipeline\n",
    "The kwargs for clustering algorithms are algorithm specific.\n",
    "Outer arguments are used for \"global\" variables and pipeline building:\n",
    "- scaler: choose which scaling factor to use, or None\n",
    "- dim_reduce: choose which dimensionality reduction algorithm to use, or None\n",
    "- clusterer: choose which clustering algorithm to use.\n",
    "- n_clusters: the number of clusters to aim for during clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d97befe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_config = {\n",
    "    \"kmeans_kwargs\": {\n",
    "        \"init\": \"k-means++\",\n",
    "        \"n_init\": 50,\n",
    "        \"max_iter\": 500,\n",
    "        \"random_state\": 2901,\n",
    "    },\n",
    "    \"hdbscan_kwargs\": {\n",
    "        'min_cluster_size': 15,\n",
    "        'min_samples': 4,\n",
    "        'metric': 'euclidean',\n",
    "    },\n",
    "    \"agglom_kwargs\": {\n",
    "        'metric': 'euclidean',\n",
    "        'linkage': 'ward',\n",
    "    },\n",
    "    \"cluster\": \"kmeans\", # kmeans, hdbscan, agglom\n",
    "    \"scaler\": \"minmax\", # std, minmax\n",
    "    \"dim_reduce\": \"pca\", # pca, umap\n",
    "    \"n_clusters\": 7,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb854376",
   "metadata": {},
   "source": [
    "### See how the data is grouped after dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33e0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_red = cluster.Cluster()\n",
    "cluster_config['dim_reduce'] = 'pca'\n",
    "pipe = dim_red.setup_pipeline(**cluster_config)\n",
    "pipe.fit(feats)\n",
    "df = dim_red.run_pipeline(pipe, feats, orig_df['Cluster'])\n",
    "dim_red.show_simple_scatterplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c68885a",
   "metadata": {},
   "source": [
    "The above graph shows that PCA has grouped data into dense clusters of varying shapes. HDBSCAN would be a good candidate clustering algorithm for this dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a981488",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_red = cluster.Cluster()\n",
    "cluster_config['dim_reduce'] = 'umap'\n",
    "pipe = dim_red.setup_pipeline(**cluster_config)\n",
    "pipe.fit(feats)\n",
    "df = dim_red.run_pipeline(pipe, feats, orig_df['Cluster'])\n",
    "dim_red.show_simple_scatterplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62710ae1",
   "metadata": {},
   "source": [
    "The above graph shows that UMAP has grouped data into more convex-looking clusters. k-means would be a good candidate clustering algorithm for this dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c138a7",
   "metadata": {},
   "source": [
    "### Find the best number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba28ba0",
   "metadata": {},
   "source": [
    "Let's find the best number of clusters for k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d227d185",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_config['cluster'] = 'kmeans'\n",
    "cluster_config['dim_reduce'] = 'umap'\n",
    "cl.find_best_n_components(feats, cluster_config, orig_df['Cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db1b1a3",
   "metadata": {},
   "source": [
    "Looks like the best value for `n_components` could be 7.\n",
    "Let's search over hyperparameters for HDBSCAN to get as close as possible to `n_components = 7`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced78158",
   "metadata": {},
   "source": [
    "Running grid search for HDBSCAN..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad2f0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "n_rows = 5\n",
    "n_cols = 5\n",
    "fig, ax = plt.subplots(nrows=n_rows, ncols=n_cols, sharex=True, sharey=True, figsize=(12, 12))\n",
    "subplot_i = 1\n",
    "cluster_config['cluster'] = 'hdbscan'\n",
    "cluster_config['dim_reduce'] = 'pca'\n",
    "\n",
    "for i, (x, y) in enumerate(list(itertools.product(range(9, 24, 3), (range(1, 16, 3))))):\n",
    "    cluster_config['hdbscan_kwargs']['min_cluster_size'] = x\n",
    "    cluster_config['hdbscan_kwargs']['min_samples'] = y\n",
    "    pipe = cl.setup_pipeline(**cluster_config)\n",
    "    pipe.fit(feats)\n",
    "    df = cl.run_pipeline(pipe, feats, orig_df['Cluster'])\n",
    "    n_clusters = len(df['predicted_cluster'].unique())-1\n",
    "\n",
    "    ax[i//n_cols][i%n_cols].plot([-1, 1], [-1.6, -1.6], color='k', lw=2)\n",
    "    plt.subplot(n_rows, n_cols, subplot_i)\n",
    "    plt.title(f\"({x}, {y}), size {n_clusters}\", fontsize='small')\n",
    "    plt.scatter(df['component_1'], df['component_2'], marker='o', c=df['predicted_cluster'], s=25, edgecolors='k')\n",
    "\n",
    "    subplot_i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86218b6b",
   "metadata": {},
   "source": [
    "Looks like `min_cluster_size=15` and `min_samples=4` are good values for the hyperparameters, giving us a cluster size of 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21999608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some new variables\n",
    "cluster_config['n_clusters'] = 7\n",
    "cluster_config['hdbscan_kwargs']['min_cluster_size'] = 15\n",
    "cluster_config['hdbscan_kwargs']['min_samples'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c1b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_config['cluster'] = 'hdbscan'\n",
    "cluster_config['dim_reduce'] = 'pca'\n",
    "cl.find_best_n_components(feats, cluster_config, orig_df['Cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283bb8fb",
   "metadata": {},
   "source": [
    "We can see that performance is fairly good at 7 clusters for HDBSCAN as well. Even though performance increases with more components, I want to keep analysis of cluster simpler for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e9b187",
   "metadata": {},
   "source": [
    "### Choose a clustering algorithm and set up the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab77af4",
   "metadata": {},
   "source": [
    "We start with k-means + UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08b3b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_config['cluster'] = 'kmeans'\n",
    "cluster_config['dim_reduce'] = 'umap'\n",
    "pipe = cl.setup_pipeline(**cluster_config)\n",
    "pipe[\"dim_reducer\"][cluster_config['dim_reduce']].n_components = 2\n",
    "\n",
    "pipe.fit(feats)\n",
    "\n",
    "kmeans_df = cl.run_pipeline(pipe, feats, orig_df['Cluster'])\n",
    "kmeans_labels = cl.pred_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55241537",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.show_scatterplot(kmeans_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9527253",
   "metadata": {},
   "source": [
    "Now we do the same for HDBSCAN + PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160caf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_config['cluster'] = 'hdbscan'\n",
    "cluster_config['dim_reduce'] = 'pca'\n",
    "pipe = cl.setup_pipeline(**cluster_config)\n",
    "pipe[\"dim_reducer\"][cluster_config['dim_reduce']].n_components = 2\n",
    "\n",
    "pipe.fit(feats)\n",
    "\n",
    "hdbscan_df = cl.run_pipeline(pipe, feats, orig_df['Cluster'])\n",
    "hdbscan_labels = cl.pred_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9d4e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.show_scatterplot(hdbscan_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f298b8",
   "metadata": {},
   "source": [
    "## Analyze and create new clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea7ef6a",
   "metadata": {},
   "source": [
    "The following graphs show top extracted words using NMF, PLSA, and LDA. These top words can be used to create or enhance clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1684327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = cl.topic_analysis(orig_df['Title'], feats, n_components=7, n_top_words=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d62bebe",
   "metadata": {},
   "source": [
    "We can use LLMs to help us categorize these groups of top words. The output of the LLM will not be directly used as a cluster definition, but rather as a starting point for generating ideas about how to define the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210ab57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "categories = {}\n",
    "instruction_prompt = \"You are a helpful chatbot. Use only the following pieces of context to answer the question. Don't make up any new information:\\n\"\n",
    "user_prompt = \"Categorize the following group of data into one category, using as few words as possible:\\n\"\n",
    "\n",
    "for model, data in top_words.items():\n",
    "    categories[model] = []\n",
    "\n",
    "    for d in data:\n",
    "        response = ollama.chat(\n",
    "            model='hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF',\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': instruction_prompt},\n",
    "                {'role': 'user', 'content': user_prompt + ','.join(d)},\n",
    "            ],\n",
    "        )\n",
    "        categories[model].append(response['message']['content'])\n",
    "\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c429faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put top words into a dataframe for easier visual comparison\n",
    "tw_df = pd.DataFrame.from_dict(top_words).explode(['nmf', 'plsa', 'lda']).reset_index()\n",
    "tw_nmf_df = tw_df[['index', 'nmf']].sort_values(by=['index', 'nmf']).reset_index(drop=True)\n",
    "tw_plsa_df = tw_df[['index', 'plsa']].sort_values(by=['index', 'plsa']).reset_index(drop=True)\n",
    "tw_lda_df = tw_df[['index', 'lda']].sort_values(by=['index', 'lda']).reset_index(drop=True)\n",
    "tw_df = tw_nmf_df[['nmf']].join(tw_plsa_df, how='left')\n",
    "tw_df = tw_df[['nmf', 'plsa']].join(tw_lda_df, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a3dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_df[tw_df['index'] == 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1128210c",
   "metadata": {},
   "source": [
    "Taking these categories, together with the top words, we can define our own clusters. These cluster definitions and their associated top words can then be used in a KNN algorithm to assign new clusters to our original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec09159",
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_categories = {\n",
    "    'nmf': [\n",
    "        'Marketing',\n",
    "        'Product Management',\n",
    "        'Leadership',\n",
    "        'Data Analytics',\n",
    "        'Leadership',\n",
    "        'IT Engineering',\n",
    "        'Data Analytics',\n",
    "    ],\n",
    "    'plsa': [\n",
    "        'Product Management',\n",
    "        'Product Management',\n",
    "        'Leadership',\n",
    "        'Data Analytics',\n",
    "        'Product Management',\n",
    "        'IT Engineering',\n",
    "        'Data Analytics',\n",
    "    ],\n",
    "    'lda': [\n",
    "        'Data Analytics',\n",
    "        'Leadership',\n",
    "        'Data Analytics',\n",
    "        'Product Management',\n",
    "        'Product Management',\n",
    "        'Data Analytics',\n",
    "        'Leadership',\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082bdd46",
   "metadata": {},
   "source": [
    "First, we compile the top words from our topic extraction into feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c30dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_by_cat = {}\n",
    "top_cats = set()\n",
    "# remove words that appear across multiple topics\n",
    "# this is a hand-curated list of words that are ambiguous or possibly semantically vacuous in this context\n",
    "remove_words = [\n",
    "    'executive', 'vice', 'manager', 'senior', 'head', 'director', 'president', 'chief'\n",
    "]\n",
    "total_categories = 0\n",
    "\n",
    "# create a list of words under each new curated category based on the extracted words from each topic extraction model\n",
    "for model, cats in curated_categories.items():\n",
    "    for i, cat in enumerate(cats):\n",
    "        if cat not in top_words_by_cat:\n",
    "            top_words_by_cat[cat] = set()\n",
    "            top_cats.update([cat])\n",
    "            total_categories += 1\n",
    "        \n",
    "        top_words_by_cat[cat].update([w for w in top_words[model][i] if w not in remove_words])\n",
    "\n",
    "top_cats = list(top_cats)\n",
    "top_cats.sort()\n",
    "top_words_feats = np.zeros((len(cl.vocab), total_categories))\n",
    "\n",
    "# create a feature vector of the top words based on the vocabulary from the original data\n",
    "for i in range(total_categories):\n",
    "    curr_cat = top_cats[i]\n",
    "\n",
    "    for word in top_words_by_cat[curr_cat]:\n",
    "        top_words_feats[cl.vocab.index(word)][i] = 1\n",
    "\n",
    "top_words_feats = top_words_feats.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3094a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345568d4",
   "metadata": {},
   "source": [
    "Then, we scale the features and run dimensionality reduction (I am using PCA and UMAP for each clustering algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83045b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from umap import UMAP\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "pca = PCA(n_components=2, random_state=2901)\n",
    "ump = UMAP(n_components=2, n_neighbors=5, random_state=2901)\n",
    "\n",
    "top_words_feats_pca = top_words_feats.copy()\n",
    "top_words_feats_pca = pca.fit_transform(top_words_feats_pca)\n",
    "top_words_feats_pca = scaler.fit_transform(top_words_feats_pca)\n",
    "top_words_pca_df = pd.DataFrame(top_words_feats_pca)\n",
    "\n",
    "top_words_feats_ump = top_words_feats.copy()\n",
    "top_words_feats_ump = ump.fit_transform(top_words_feats_ump)\n",
    "top_words_feats_ump = scaler.fit_transform(top_words_feats_ump)\n",
    "top_words_ump_df = pd.DataFrame(top_words_feats_ump)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d11864",
   "metadata": {},
   "source": [
    "The scatterplots show where the centroid of each category resides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa4d28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=top_words_pca_df[0], y=top_words_pca_df[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d8cb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=top_words_ump_df[0], y=top_words_ump_df[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58165db",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = orig_df.copy()\n",
    "new_df['new_pca_cluster'] = 'Other'\n",
    "new_df['new_ump_cluster'] = 'Other'\n",
    "new_pca_feats = pca.fit_transform(feats)\n",
    "new_pca_feats = scaler.fit_transform(new_pca_feats)\n",
    "new_ump_feats = ump.fit_transform(feats)\n",
    "new_ump_feats = scaler.fit_transform(new_ump_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8d223f",
   "metadata": {},
   "source": [
    "Finally, we can set up the KNN classifier. I tried out different values for `n_neighbors`, `weights`, and `metric` that eventually gave me some reasonable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfbe738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pca_knn = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "ump_knn = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "\n",
    "pca_knn.fit(top_words_feats_pca, top_cats)\n",
    "ump_knn.fit(top_words_feats_ump, top_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4de47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, point in enumerate(new_pca_feats):\n",
    "    pred_lbl = pca_knn.predict([tuple(point)])[0]\n",
    "    new_df['new_pca_cluster'].at[idx] = pred_lbl\n",
    "\n",
    "for idx, point in enumerate(new_ump_feats):\n",
    "    pred_lbl = ump_knn.predict([tuple(point)])[0]\n",
    "    new_df['new_ump_cluster'].at[idx] = pred_lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86fe9bd",
   "metadata": {},
   "source": [
    "Save the new clusters to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469ab901",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('../data/new_clusters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fafa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load existing clusters\n",
    "new_df = pd.read_csv('../data/new_clusters.csv')\n",
    "# keep only 2 columns\n",
    "new_df = new_df[['Title', 'new_pca_cluster', 'new_ump_cluster']]\n",
    "\n",
    "ncl = cluster.Cluster()\n",
    "# pass in the raw data to get out features\n",
    "nfeats = ncl.get_features(new_df['Title'], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9747744d",
   "metadata": {},
   "source": [
    "Here are a couple of graphs showing the performance of k-means and HDBSCAN over the data now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1af1669",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_config['cluster'] = 'kmeans'\n",
    "cluster_config['dim_reduce'] = 'umap'\n",
    "ncl.find_best_n_components(nfeats, cluster_config, new_df['new_ump_cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fb75fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_config['cluster'] = 'hdbscan'\n",
    "cluster_config['dim_reduce'] = 'pca'\n",
    "ncl.find_best_n_components(nfeats, cluster_config, new_df['new_pca_cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a189db",
   "metadata": {},
   "source": [
    "Both graphs look like there are much better results this time!\n",
    "\n",
    "We can also look at the clustering results again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025297ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_config['cluster'] = 'kmeans'\n",
    "cluster_config['dim_reduce'] = 'umap'\n",
    "cluster_config['n_clusters'] = 5\n",
    "new_pipe = ncl.setup_pipeline(**cluster_config)\n",
    "new_pipe[\"dim_reducer\"][cluster_config['dim_reduce']].n_components = 2\n",
    "\n",
    "new_pipe.fit(nfeats)\n",
    "\n",
    "new_kmeans_df = ncl.run_pipeline(new_pipe, nfeats, new_df['new_ump_cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24d35cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncl.show_scatterplot(new_kmeans_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15143c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_config['cluster'] = 'hdbscan'\n",
    "cluster_config['dim_reduce'] = 'pca'\n",
    "cluster_config['hdbscan_kwargs']['min_cluster_size'] = 20\n",
    "cluster_config['hdbscan_kwargs']['min_samples'] = 3\n",
    "new_pipe = ncl.setup_pipeline(**cluster_config)\n",
    "new_pipe[\"dim_reducer\"][cluster_config['dim_reduce']].n_components = 2\n",
    "\n",
    "new_pipe.fit(nfeats)\n",
    "\n",
    "new_hdbscan_df = ncl.run_pipeline(new_pipe, nfeats, new_df['new_pca_cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92a9deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncl.show_scatterplot(new_hdbscan_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466a8a2a",
   "metadata": {},
   "source": [
    "I noticed how HDBSCAN is having trouble with the less dense groupings of data near the top and bottom of the graph, so I decided to try an Agglomerative Clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfbf540",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_config['cluster'] = 'agglom'\n",
    "cluster_config['dim_reduce'] = 'pca'\n",
    "cluster_config['n_clusters'] = 4\n",
    "new_pipe = ncl.setup_pipeline(**cluster_config)\n",
    "new_pipe[\"dim_reducer\"][cluster_config['dim_reduce']].n_components = 2\n",
    "\n",
    "new_pipe.fit(nfeats)\n",
    "\n",
    "new_agglom_df = ncl.run_pipeline(new_pipe, nfeats, new_df['new_pca_cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b211f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncl.show_scatterplot(new_agglom_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e20009",
   "metadata": {},
   "source": [
    "The groupings look much better with the Agglomerative Clustering algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba860b4e",
   "metadata": {},
   "source": [
    "Also of note is that the \"IT Engineering\" cluster is not shown in the graphs that used PCA. It is most likely that those data points were not picked up during KNN calculations because of the dimensionality reduction process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a455a476",
   "metadata": {},
   "source": [
    "It looks like the clusterings are more homogeneous now, but additional work needs to be done to better define these topics. Next steps could be to investigate the clusters for patterns, and/or to involve subject-matter experts to refine definitions. We can also try out different cluster sizes and number of clusters, given that the definition and distribution of the data may have changed during analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8652d9f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
